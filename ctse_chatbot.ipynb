{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a972c2fd-c310-40b6-8f40-fbddb4d17cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b5539d-a026-4e6b-bb04-75ebc3ec1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function to convert a single pptx file to txt\n",
    "def pptx_to_txt(pptx_path, txt_output_path):\n",
    "    prs = Presentation(pptx_path)\n",
    "    text = \"\"\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \"\\n\"\n",
    "\n",
    "    with open(txt_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "# 4. Convert all .pptx files in a folder\n",
    "def convert_all_pptx_to_txt(pptx_folder_path, output_folder_path):\n",
    "    # Make sure output folder exists\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(pptx_folder_path):\n",
    "        if filename.endswith(\".pptx\"):\n",
    "            pptx_path = os.path.join(pptx_folder_path, filename)\n",
    "            txt_filename = filename.replace(\".pptx\", \".txt\")\n",
    "            txt_output_path = os.path.join(output_folder_path, txt_filename)\n",
    "            pptx_to_txt(pptx_path, txt_output_path)\n",
    "            print(f\"Converted {filename} -> {txt_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d37ab5-c305-4cc5-8114-fc5451c84aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Set your paths\n",
    "pptx_folder = \"./\"  # If the .pptx files are in the same folder as your notebook\n",
    "output_folder = \"./converted_txt\"  # Save .txt files in a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b00f95-7bdd-4678-95d9-997695d547be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted AWS User Groups Colombo - Introduction to AWS Cloud Platform.pptx -> AWS User Groups Colombo - Introduction to AWS Cloud Platform.txt\n",
      "Converted CAP Theorem.pptx -> CAP Theorem.txt\n",
      "Converted Cloud Computing 101.pptx -> Cloud Computing 101.txt\n",
      "Converted Containers 101.pptx -> Containers 101.txt\n",
      "Converted Intro to DevOps and Beyond.pptx -> Intro to DevOps and Beyond.txt\n",
      "Converted Lecture 2 - Part 1.pptx -> Lecture 2 - Part 1.txt\n",
      "Converted Lecture 2 - Part 2.pptx -> Lecture 2 - Part 2.txt\n",
      "✅ All PPTX files converted to TXT files successfully!\n"
     ]
    }
   ],
   "source": [
    "# 6. Run the conversion\n",
    "convert_all_pptx_to_txt(pptx_folder, output_folder)\n",
    "\n",
    "print(\"✅ All PPTX files converted to TXT files successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f28e9f-ed8b-4dbd-a429-02392e549993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All TXT files combined into ctse_notes.txt!\n"
     ]
    }
   ],
   "source": [
    "# Combine all converted txt files into a single file\n",
    "def combine_txt_files(input_folder, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "                    outfile.write(\"\\n\\n\")  # Add some space between documents\n",
    "\n",
    "combine_txt_files(\"./converted_txt\", \"ctse_notes.txt\")\n",
    "print(\"✅ All TXT files combined into ctse_notes.txt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb62533c-c606-49f7-8c76-e23092bf72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65c00263-fe7f-4aaf-ad1d-1e2acf8d9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CTSE lecture notes\n",
    "loader = TextLoader(\"ctse_notes.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "603d76b3-8fbc-4395-9009-88e04994c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80db3834-044f-48d9-8798-1716ac44965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56310b34-5581-4989-ad3a-c1f254064397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e3ee2bd-5167-4042-a2c9-1c179d523b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup the Hugging Face LLM locally\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4433e373-99dc-4be0-bf67-b182c440a705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a **text2text** generation pipeline correctly\n",
    "text2text_generator = pipeline(\n",
    "    \"text2text-generation\",   # use correct task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1  # -1 means CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2afd5665-98e6-4fa4-8529-635222695a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_16008\\291416619.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text2text_generator, model_kwargs={\"temperature\":0})\n"
     ]
    }
   ],
   "source": [
    "# Now wrap the local pipeline properly\n",
    "llm = HuggingFacePipeline(pipeline=text2text_generator, model_kwargs={\"temperature\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a6d5bf3-60a5-4aa5-9965-fcfc17bb27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "399f8d3e-ff79-4601-8764-fa22b8d8e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ask questions\n",
    "def answer_query(query):\n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    print(f\"\\nQuestion: {query}\\nAnswer: {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3b08bfc-38ec-46f7-a852-5666367bac42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is container?\n",
      "Answer: a platform to manage lifecycle of your containers, Develop your apps and supporting components using containers\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What is container?\"\n",
    "    answer_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d4a00-4b95-4d48-8d8c-b563253295c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
